{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de Pré-processamento de Texto para NLP\n",
    "\n",
    "Este notebook demonstra várias técnicas de pré-processamento de texto utilizando Python, aplicadas a uma base de dados de um projeto de NLP para a Uber. O objetivo é aplicar três técnicas diferentes de pré-processamento em 10 frases da base de dados, mostrando os resultados de cada técnica e construindo um pipeline completo de pré-processamento.\n",
    "\n",
    "O projeto faz parte de um estudo maior disponível no repositório [Projeto - Uber NLP](https://github.com/2024M6T4-Inteli/Projeto3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "# Aqui você adicionaria o código para carregar a base de dados específica\n",
    "\n",
    "# Exemplo de carregamento de dados:\n",
    "data = [\"Frase 1\", \"Frase 2\", \"Frase 3\", \"Frase 4\", \"Frase 5\",\n",
    "        \"Frase 6\", \"Frase 7\", \"Frase 8\", \"Frase 9\", \"Frase 10\"]\n",
    "\n",
    "# Exibindo as frases\n",
    "for i, frase in enumerate(data):\n",
    "    print(f\"{i}: {frase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remoção de Stopwords\n",
    "\n",
    "**Descrição:** A remoção de stopwords elimina palavras comuns que não adicionam significado relevante ao texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Removendo stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentences = []\n",
    "\n",
    "for sentence in data:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
    "    filtered_sentences.append(' '.join(filtered_sentence))\n",
    "\n",
    "filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenização\n",
    "\n",
    "**Descrição:** Tokenização é o processo de dividir o texto em palavras ou frases menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizando frases\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in data]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "\n",
    "**Descrição:** Stemming reduz as palavras às suas raízes ou formas base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicando Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_sentences = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    stemmed_sentence = [stemmer.stem(word) for word in sentence]\n",
    "    stemmed_sentences.append(' '.join(stemmed_sentence))\n",
    "\n",
    "stemmed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação \"From Scratch\": Normalização de Texto\n",
    "\n",
    "**Descrição:** Implementação da normalização de texto sem o uso de bibliotecas externas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Normalização de texto \"from scratch\"\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "normalized_sentences = [normalize_text(sentence) for sentence in data]\n",
    "normalized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do Pipeline de Pré-processamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Construindo o pipeline de pré-processamento\n",
    "def preprocess_pipeline(sentences):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Normalização\n",
    "        sentence = normalize_text(sentence)\n",
    "        \n",
    "        # Tokenização\n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        # Remoção de Stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Stemming\n",
    "        stemmed_sentence = [stemmer.stem(word) for word in words]\n",
    "        \n",
    "        processed_sentences.append(' '.join(stemmed_sentence))\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "# Aplicando o pipeline\n",
    "final_sentences = preprocess_pipeline(data)\n",
    "final_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados e Conclusão\n",
    "\n",
    "Os resultados mostram as diferentes transformações aplicadas aos textos de exemplo, demonstrando as técnicas de pré-processamento de NLP e suas utilidades na preparação de dados para modelos de aprendizado de máquina.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
